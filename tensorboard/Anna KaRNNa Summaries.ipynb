{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anna KaRNNa\n",
    "\n",
    "In this notebook, I'll build a character-wise RNN trained on Anna Karenina, one of my all-time favorite books. It'll be able to generate new text based on the text from the book.\n",
    "\n",
    "This network is based off of Andrej Karpathy's [post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [implementation in Torch](https://github.com/karpathy/char-rnn). Also, some information [here at r2rt](http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html) and from [Sherjil Ozair](https://github.com/sherjilozair/char-rnn-tensorflow) on GitHub. Below is the general architecture of the character-wise RNN.\n",
    "\n",
    "<img src=\"assets/charseq.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll load the text file and convert it into integers for our network to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "vocab = set(text)\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "chars = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13, 70,  8, 43, 81,  0, 23, 21, 64, 77, 77, 77, 27,  8, 43, 43, 22,\n",
       "       21, 82,  8,  2, 16, 65, 16,  0, 37, 21,  8, 23,  0, 21,  8, 65, 65,\n",
       "       21,  8, 65, 16, 44,  0, 15, 21,  0, 34,  0, 23, 22, 21, 25, 59, 70,\n",
       "        8, 43, 43, 22, 21, 82,  8,  2, 16, 65, 22, 21, 16, 37, 21, 25, 59,\n",
       "       70,  8, 43, 43, 22, 21, 16, 59, 21, 16, 81, 37, 21, 24, 75, 59, 77,\n",
       "       75,  8, 22, 72, 77, 77, 56, 34,  0, 23, 22, 81, 70, 16, 59])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I need to split up the data into batches, and into training and validation sets. I should be making a test set here, but I'm not going to worry about that. My test will be if the network can generate new text.\n",
    "\n",
    "Here I'll make both input and target arrays. The targets are the same as the inputs, except shifted one character over. I'll also drop the last bit of data so that I'll only have completely full batches.\n",
    "\n",
    "The idea here is to make a 2D matrix where the number of rows is equal to the number of batches. Each row will be one long concatenated string from the character data. We'll split this data into a training set and validation set using the `split_frac` keyword. This will keep 90% of the batches in the training set, the other 10% in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(chars, batch_size, num_steps, split_frac=0.9):\n",
    "    \"\"\" \n",
    "    Split character data into training and validation sets, inputs and targets for each set.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    chars: character array\n",
    "    batch_size: Size of examples in each of batch\n",
    "    num_steps: Number of sequence steps to keep in the input and pass to the network\n",
    "    split_frac: Fraction of batches to keep in the training set\n",
    "    \n",
    "    \n",
    "    Returns train_x, train_y, val_x, val_y\n",
    "    \"\"\"\n",
    "    \n",
    "    slice_size = batch_size * num_steps\n",
    "    n_batches = int(len(chars) / slice_size)\n",
    "    \n",
    "    # Drop the last few characters to make only full batches\n",
    "    x = chars[: n_batches*slice_size]\n",
    "    y = chars[1: n_batches*slice_size + 1]\n",
    "    \n",
    "    # Split the data into batch_size slices, then stack them into a 2D matrix \n",
    "    x = np.stack(np.split(x, batch_size))\n",
    "    y = np.stack(np.split(y, batch_size))\n",
    "    \n",
    "    # Now x and y are arrays with dimensions batch_size x n_batches*num_steps\n",
    "    \n",
    "    # Split into training and validation sets, keep the virst split_frac batches for training\n",
    "    split_idx = int(n_batches*split_frac)\n",
    "    train_x, train_y= x[:, :split_idx*num_steps], y[:, :split_idx*num_steps]\n",
    "    val_x, val_y = x[:, split_idx*num_steps:], y[:, split_idx*num_steps:]\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_x, train_y, val_x, val_y = split_data(chars, 10, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 178400)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13, 70,  8, 43, 81,  0, 23, 21, 64, 77],\n",
       "       [52, 59, 58, 21, 70,  0, 21,  2, 24, 34],\n",
       "       [21,  3,  8, 81,  3, 70, 16, 59, 79, 21],\n",
       "       [24, 81, 70,  0, 23, 21, 75, 24, 25, 65],\n",
       "       [21, 81, 70,  0, 21, 65,  8, 59, 58, 60],\n",
       "       [21, 55, 70, 23, 24, 25, 79, 70, 21, 65],\n",
       "       [81, 21, 81, 24, 77, 58, 24, 72, 77, 77],\n",
       "       [24, 21, 70,  0, 23, 37,  0, 65, 82,  6],\n",
       "       [70,  8, 81, 21, 16, 37, 21, 81, 70,  0],\n",
       "       [ 0, 23, 37,  0, 65, 82, 21,  8, 59, 58]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[:,:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll write another function to grab batches out of the arrays made by split data. Here each batch will be a sliding window on these arrays with size `batch_size X num_steps`. For example, if we want our network to train on a sequence of 100 characters, `num_steps = 100`. For the next batch, we'll shift this window the next sequence of `num_steps` characters. In this way we can feed batches to the network and the cell states will continue through on each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(arrs, num_steps):\n",
    "    batch_size, slice_size = arrs[0].shape\n",
    "    \n",
    "    n_batches = int(slice_size/num_steps)\n",
    "    for b in range(n_batches):\n",
    "        yield [x[:, b*num_steps: (b+1)*num_steps] for x in arrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_rnn(num_classes, batch_size=50, num_steps=50, lstm_size=128, num_layers=2,\n",
    "              learning_rate=0.001, grad_clip=5, sampling=False):\n",
    "        \n",
    "    if sampling == True:\n",
    "        batch_size, num_steps = 1, 1\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    with tf.name_scope('inputs'):\n",
    "        inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "        x_one_hot = tf.one_hot(inputs, num_classes, name='x_one_hot')\n",
    "    \n",
    "    with tf.name_scope('targets'):\n",
    "        targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
    "        y_one_hot = tf.one_hot(targets, num_classes, name='y_one_hot')\n",
    "        y_reshaped = tf.reshape(y_one_hot, [-1, num_classes])\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    # Build the RNN layers\n",
    "    with tf.name_scope(\"RNN_cells\"):\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "    \n",
    "    with tf.name_scope(\"RNN_init_state\"):\n",
    "        initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    # Run the data through the RNN layers\n",
    "    with tf.name_scope(\"RNN_forward\"):\n",
    "        rnn_inputs = [tf.squeeze(i, squeeze_dims=[1]) for i in tf.split(x_one_hot, num_steps, 1)]\n",
    "        outputs, state = tf.contrib.rnn.static_rnn(cell, rnn_inputs, initial_state=initial_state)\n",
    "    \n",
    "    final_state = state\n",
    "    \n",
    "    # Reshape output so it's a bunch of rows, one row for each cell output\n",
    "    with tf.name_scope('sequence_reshape'):\n",
    "        seq_output = tf.concat(outputs, axis=1,name='seq_output')\n",
    "        output = tf.reshape(seq_output, [-1, lstm_size], name='graph_output')\n",
    "    \n",
    "    # Now connect the RNN outputs to a softmax layer and calculate the cost\n",
    "    with tf.name_scope('logits'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((lstm_size, num_classes), stddev=0.1),\n",
    "                               name='softmax_w')\n",
    "        softmax_b = tf.Variable(tf.zeros(num_classes), name='softmax_b')\n",
    "        logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "        tf.summary.histogram('softmax_w', softmax_w)\n",
    "        tf.summary.histogram('softmax_b', softmax_b)\n",
    "\n",
    "    with tf.name_scope('predictions'):\n",
    "        preds = tf.nn.softmax(logits, name='predictions')\n",
    "        tf.summary.histogram('predictions', preds)\n",
    "    \n",
    "    with tf.name_scope('cost'):\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped, name='loss')\n",
    "        cost = tf.reduce_mean(loss, name='cost')\n",
    "        tf.summary.scalar('cost', cost)\n",
    "\n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    with tf.name_scope('train'):\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "        optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    # Export the nodes \n",
    "    export_nodes = ['inputs', 'targets', 'initial_state', 'final_state',\n",
    "                    'keep_prob', 'cost', 'preds', 'optimizer', 'merged']\n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Here I'm defining the hyperparameters for the network. The two you probably haven't seen before are `lstm_size` and `num_layers`. These set the number of hidden units in the LSTM layers and the number of LSTM layers, respectively. Of course, making these bigger will improve the network's performance but you'll have to watch out for overfitting. If your validation loss is much larger than the training loss, you're probably overfitting. Decrease the size of the network or decrease the dropout keep probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_steps = 100\n",
    "lstm_size = 512\n",
    "num_layers = 2\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Time for training which is is pretty straightforward. Here I pass in some data, and get an LSTM state back. Then I pass that state back in to the network so the next batch can continue the state from the previous batch. And every so often (set by `save_every_n`) I calculate the validation loss and save a checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The syntax of the command is incorrect.\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p checkpoints/anna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10  Iteration 1/1780 Training loss: 4.4188 22.9383 sec/batch\n",
      "Epoch 1/10  Iteration 2/1780 Training loss: 4.3738 15.6873 sec/batch\n",
      "Epoch 1/10  Iteration 3/1780 Training loss: 4.1817 16.2784 sec/batch\n",
      "Epoch 1/10  Iteration 4/1780 Training loss: 4.2735 16.6417 sec/batch\n",
      "Epoch 1/10  Iteration 5/1780 Training loss: 4.2706 16.9847 sec/batch\n",
      "Epoch 1/10  Iteration 6/1780 Training loss: 4.2172 16.4340 sec/batch\n",
      "Epoch 1/10  Iteration 7/1780 Training loss: 4.1500 23.2466 sec/batch\n",
      "Epoch 1/10  Iteration 8/1780 Training loss: 4.0757 16.2020 sec/batch\n",
      "Epoch 1/10  Iteration 9/1780 Training loss: 4.0080 15.7963 sec/batch\n",
      "Epoch 1/10  Iteration 10/1780 Training loss: 3.9516 17.7096 sec/batch\n",
      "Epoch 1/10  Iteration 11/1780 Training loss: 3.9014 18.1382 sec/batch\n",
      "Epoch 1/10  Iteration 12/1780 Training loss: 3.8585 18.5948 sec/batch\n",
      "Epoch 1/10  Iteration 13/1780 Training loss: 3.8209 18.2871 sec/batch\n",
      "Epoch 1/10  Iteration 14/1780 Training loss: 3.7879 18.6086 sec/batch\n",
      "Epoch 1/10  Iteration 15/1780 Training loss: 3.7573 19.4945 sec/batch\n",
      "Epoch 1/10  Iteration 16/1780 Training loss: 3.7298 18.5366 sec/batch\n",
      "Epoch 1/10  Iteration 17/1780 Training loss: 3.7047 17.1832 sec/batch\n",
      "Epoch 1/10  Iteration 18/1780 Training loss: 3.6835 16.6396 sec/batch\n",
      "Epoch 1/10  Iteration 19/1780 Training loss: 3.6625 15.9170 sec/batch\n",
      "Epoch 1/10  Iteration 20/1780 Training loss: 3.6425 14.9109 sec/batch\n",
      "Epoch 1/10  Iteration 21/1780 Training loss: 3.6251 13.8045 sec/batch\n",
      "Epoch 1/10  Iteration 22/1780 Training loss: 3.6087 13.6225 sec/batch\n",
      "Epoch 1/10  Iteration 23/1780 Training loss: 3.5932 14.0685 sec/batch\n",
      "Epoch 1/10  Iteration 24/1780 Training loss: 3.5787 13.0670 sec/batch\n",
      "Epoch 1/10  Iteration 25/1780 Training loss: 3.5649 13.0635 sec/batch\n",
      "Epoch 1/10  Iteration 26/1780 Training loss: 3.5527 14.5111 sec/batch\n",
      "Epoch 1/10  Iteration 27/1780 Training loss: 3.5414 12.9943 sec/batch\n",
      "Epoch 1/10  Iteration 28/1780 Training loss: 3.5302 12.9860 sec/batch\n",
      "Epoch 1/10  Iteration 29/1780 Training loss: 3.5197 12.9605 sec/batch\n",
      "Epoch 1/10  Iteration 30/1780 Training loss: 3.5100 16.5365 sec/batch\n",
      "Epoch 1/10  Iteration 31/1780 Training loss: 3.5015 167.7317 sec/batch\n",
      "Epoch 1/10  Iteration 32/1780 Training loss: 3.4924 19.3680 sec/batch\n",
      "Epoch 1/10  Iteration 33/1780 Training loss: 3.4834 18.1338 sec/batch\n",
      "Epoch 1/10  Iteration 34/1780 Training loss: 3.4754 15.7102 sec/batch\n",
      "Epoch 1/10  Iteration 35/1780 Training loss: 3.4672 17.8696 sec/batch\n",
      "Epoch 1/10  Iteration 36/1780 Training loss: 3.4600 16.1972 sec/batch\n",
      "Epoch 1/10  Iteration 37/1780 Training loss: 3.4524 16.4874 sec/batch\n",
      "Epoch 1/10  Iteration 38/1780 Training loss: 3.4452 15.2161 sec/batch\n",
      "Epoch 1/10  Iteration 39/1780 Training loss: 3.4383 15.2106 sec/batch\n",
      "Epoch 1/10  Iteration 40/1780 Training loss: 3.4319 17.3985 sec/batch\n",
      "Epoch 1/10  Iteration 41/1780 Training loss: 3.4254 13.5390 sec/batch\n",
      "Epoch 1/10  Iteration 42/1780 Training loss: 3.4194 14.1725 sec/batch\n",
      "Epoch 1/10  Iteration 43/1780 Training loss: 3.4136 13.1590 sec/batch\n",
      "Epoch 1/10  Iteration 44/1780 Training loss: 3.4078 13.0083 sec/batch\n",
      "Epoch 1/10  Iteration 45/1780 Training loss: 3.4021 12.8010 sec/batch\n",
      "Epoch 1/10  Iteration 46/1780 Training loss: 3.3971 14.2876 sec/batch\n",
      "Epoch 1/10  Iteration 47/1780 Training loss: 3.3925 13.0085 sec/batch\n",
      "Epoch 1/10  Iteration 48/1780 Training loss: 3.3880 13.5272 sec/batch\n",
      "Epoch 1/10  Iteration 49/1780 Training loss: 3.3837 12.7315 sec/batch\n",
      "Epoch 1/10  Iteration 50/1780 Training loss: 3.3794 12.9065 sec/batch\n",
      "Epoch 1/10  Iteration 51/1780 Training loss: 3.3752 16.3145 sec/batch\n",
      "Epoch 1/10  Iteration 52/1780 Training loss: 3.3710 14.0240 sec/batch\n",
      "Epoch 1/10  Iteration 53/1780 Training loss: 3.3672 16.8345 sec/batch\n",
      "Epoch 1/10  Iteration 54/1780 Training loss: 3.3631 17.1402 sec/batch\n",
      "Epoch 1/10  Iteration 55/1780 Training loss: 3.3594 16.8676 sec/batch\n",
      "Epoch 1/10  Iteration 56/1780 Training loss: 3.3555 14.4235 sec/batch\n",
      "Epoch 1/10  Iteration 57/1780 Training loss: 3.3520 13.1375 sec/batch\n",
      "Epoch 1/10  Iteration 58/1780 Training loss: 3.3486 13.2090 sec/batch\n",
      "Epoch 1/10  Iteration 59/1780 Training loss: 3.3450 12.8226 sec/batch\n",
      "Epoch 1/10  Iteration 60/1780 Training loss: 3.3417 13.7030 sec/batch\n",
      "Epoch 1/10  Iteration 61/1780 Training loss: 3.3385 13.2611 sec/batch\n",
      "Epoch 1/10  Iteration 62/1780 Training loss: 3.3358 13.9655 sec/batch\n",
      "Epoch 1/10  Iteration 63/1780 Training loss: 3.3332 13.1100 sec/batch\n",
      "Epoch 1/10  Iteration 64/1780 Training loss: 3.3301 12.7520 sec/batch\n",
      "Epoch 1/10  Iteration 65/1780 Training loss: 3.3271 12.9410 sec/batch\n",
      "Epoch 1/10  Iteration 66/1780 Training loss: 3.3247 14.8130 sec/batch\n",
      "Epoch 1/10  Iteration 67/1780 Training loss: 3.3221 14.0961 sec/batch\n",
      "Epoch 1/10  Iteration 68/1780 Training loss: 3.3190 445.8862 sec/batch\n",
      "Epoch 1/10  Iteration 69/1780 Training loss: 3.3163 13.3014 sec/batch\n",
      "Epoch 1/10  Iteration 70/1780 Training loss: 3.3139 12.9736 sec/batch\n",
      "Epoch 1/10  Iteration 71/1780 Training loss: 3.3113 12.7096 sec/batch\n",
      "Epoch 1/10  Iteration 72/1780 Training loss: 3.3092 12.9674 sec/batch\n",
      "Epoch 1/10  Iteration 73/1780 Training loss: 3.3068 13.1017 sec/batch\n",
      "Epoch 1/10  Iteration 74/1780 Training loss: 3.3045 12.8679 sec/batch\n",
      "Epoch 1/10  Iteration 75/1780 Training loss: 3.3024 12.8394 sec/batch\n",
      "Epoch 1/10  Iteration 76/1780 Training loss: 3.3005 12.9965 sec/batch\n",
      "Epoch 1/10  Iteration 77/1780 Training loss: 3.2984 13.2713 sec/batch\n",
      "Epoch 1/10  Iteration 78/1780 Training loss: 3.2963 12.8716 sec/batch\n",
      "Epoch 1/10  Iteration 79/1780 Training loss: 3.2942 15.0299 sec/batch\n",
      "Epoch 1/10  Iteration 80/1780 Training loss: 3.2920 60.6459 sec/batch\n",
      "Epoch 1/10  Iteration 81/1780 Training loss: 3.2899 18.4192 sec/batch\n",
      "Epoch 1/10  Iteration 82/1780 Training loss: 3.2880 15.5311 sec/batch\n",
      "Epoch 1/10  Iteration 83/1780 Training loss: 3.2861 13.4552 sec/batch\n",
      "Epoch 1/10  Iteration 84/1780 Training loss: 3.2843 14.7735 sec/batch\n",
      "Epoch 1/10  Iteration 85/1780 Training loss: 3.2822 13.0465 sec/batch\n",
      "Epoch 1/10  Iteration 86/1780 Training loss: 3.2802 12.9506 sec/batch\n",
      "Epoch 1/10  Iteration 87/1780 Training loss: 3.2782 12.7056 sec/batch\n",
      "Epoch 1/10  Iteration 88/1780 Training loss: 3.2764 13.7540 sec/batch\n",
      "Epoch 1/10  Iteration 89/1780 Training loss: 3.2747 13.9955 sec/batch\n",
      "Epoch 1/10  Iteration 90/1780 Training loss: 3.2730 13.3010 sec/batch\n",
      "Epoch 1/10  Iteration 91/1780 Training loss: 3.2713 14.0640 sec/batch\n",
      "Epoch 1/10  Iteration 92/1780 Training loss: 3.2696 44.2815 sec/batch\n",
      "Epoch 1/10  Iteration 93/1780 Training loss: 3.2679 13.5326 sec/batch\n",
      "Epoch 1/10  Iteration 94/1780 Training loss: 3.2663 14.0561 sec/batch\n",
      "Epoch 1/10  Iteration 95/1780 Training loss: 3.2645 14.2915 sec/batch\n",
      "Epoch 1/10  Iteration 96/1780 Training loss: 3.2628 13.4038 sec/batch\n",
      "Epoch 1/10  Iteration 97/1780 Training loss: 3.2612 18.2129 sec/batch\n",
      "Epoch 1/10  Iteration 98/1780 Training loss: 3.2596 16.5602 sec/batch\n",
      "Epoch 1/10  Iteration 99/1780 Training loss: 3.2579 24.5136 sec/batch\n",
      "Epoch 1/10  Iteration 100/1780 Training loss: 3.2563 18.3445 sec/batch\n",
      "Validation loss: 3.19859 Saving checkpoint!\n",
      "Epoch 1/10  Iteration 101/1780 Training loss: 3.2570 19.1438 sec/batch\n",
      "Epoch 1/10  Iteration 102/1780 Training loss: 3.2576 16.9969 sec/batch\n",
      "Epoch 1/10  Iteration 103/1780 Training loss: 3.2580 19.3998 sec/batch\n",
      "Epoch 1/10  Iteration 104/1780 Training loss: 3.2579 16.0898 sec/batch\n",
      "Epoch 1/10  Iteration 105/1780 Training loss: 3.2572 16.1296 sec/batch\n",
      "Epoch 1/10  Iteration 106/1780 Training loss: 3.2561 17.8091 sec/batch\n",
      "Epoch 1/10  Iteration 107/1780 Training loss: 3.2556 18.3767 sec/batch\n",
      "Epoch 1/10  Iteration 108/1780 Training loss: 3.2546 20.1255 sec/batch\n",
      "Epoch 1/10  Iteration 109/1780 Training loss: 3.2536 14.6190 sec/batch\n",
      "Epoch 1/10  Iteration 110/1780 Training loss: 3.2520 23.5401 sec/batch\n",
      "Epoch 1/10  Iteration 111/1780 Training loss: 3.2506 16.0392 sec/batch\n",
      "Epoch 1/10  Iteration 112/1780 Training loss: 3.2493 15.1070 sec/batch\n",
      "Epoch 1/10  Iteration 113/1780 Training loss: 3.2480 12.9640 sec/batch\n",
      "Epoch 1/10  Iteration 114/1780 Training loss: 3.2467 13.0537 sec/batch\n",
      "Epoch 1/10  Iteration 115/1780 Training loss: 3.2451 13.1175 sec/batch\n",
      "Epoch 1/10  Iteration 116/1780 Training loss: 3.2437 13.6496 sec/batch\n",
      "Epoch 1/10  Iteration 117/1780 Training loss: 3.2423 13.3698 sec/batch\n",
      "Epoch 1/10  Iteration 118/1780 Training loss: 3.2411 14.1720 sec/batch\n",
      "Epoch 1/10  Iteration 119/1780 Training loss: 3.2399 13.4170 sec/batch\n",
      "Epoch 1/10  Iteration 120/1780 Training loss: 3.2385 12.7016 sec/batch\n",
      "Epoch 1/10  Iteration 121/1780 Training loss: 3.2373 15.4765 sec/batch\n",
      "Epoch 1/10  Iteration 122/1780 Training loss: 3.2360 13.9040 sec/batch\n",
      "Epoch 1/10  Iteration 123/1780 Training loss: 3.2347 15.1975 sec/batch\n",
      "Epoch 1/10  Iteration 124/1780 Training loss: 3.2335 13.8760 sec/batch\n",
      "Epoch 1/10  Iteration 125/1780 Training loss: 3.2320 22.0401 sec/batch\n",
      "Epoch 1/10  Iteration 126/1780 Training loss: 3.2305 18.0056 sec/batch\n",
      "Epoch 1/10  Iteration 127/1780 Training loss: 3.2291 16.8102 sec/batch\n",
      "Epoch 1/10  Iteration 128/1780 Training loss: 3.2278 15.3684 sec/batch\n",
      "Epoch 1/10  Iteration 129/1780 Training loss: 3.2263 15.9185 sec/batch\n",
      "Epoch 1/10  Iteration 130/1780 Training loss: 3.2249 16.8873 sec/batch\n",
      "Epoch 1/10  Iteration 131/1780 Training loss: 3.2236 16.1656 sec/batch\n",
      "Epoch 1/10  Iteration 132/1780 Training loss: 3.2220 15.8620 sec/batch\n",
      "Epoch 1/10  Iteration 133/1780 Training loss: 3.2206 16.9626 sec/batch\n",
      "Epoch 1/10  Iteration 134/1780 Training loss: 3.2190 16.8546 sec/batch\n",
      "Epoch 1/10  Iteration 135/1780 Training loss: 3.2172 16.8942 sec/batch\n",
      "Epoch 1/10  Iteration 136/1780 Training loss: 3.2155 16.1822 sec/batch\n",
      "Epoch 1/10  Iteration 137/1780 Training loss: 3.2139 15.9220 sec/batch\n",
      "Epoch 1/10  Iteration 138/1780 Training loss: 3.2122 15.8628 sec/batch\n",
      "Epoch 1/10  Iteration 139/1780 Training loss: 3.2106 15.5985 sec/batch\n",
      "Epoch 1/10  Iteration 140/1780 Training loss: 3.2089 17.0730 sec/batch\n",
      "Epoch 1/10  Iteration 141/1780 Training loss: 3.2073 16.9835 sec/batch\n",
      "Epoch 1/10  Iteration 142/1780 Training loss: 3.2054 15.9338 sec/batch\n",
      "Epoch 1/10  Iteration 143/1780 Training loss: 3.2036 15.5716 sec/batch\n",
      "Epoch 1/10  Iteration 144/1780 Training loss: 3.2018 17.0925 sec/batch\n",
      "Epoch 1/10  Iteration 145/1780 Training loss: 3.2000 16.8077 sec/batch\n",
      "Epoch 1/10  Iteration 146/1780 Training loss: 3.1982 15.5776 sec/batch\n",
      "Epoch 1/10  Iteration 147/1780 Training loss: 3.1965 16.2928 sec/batch\n",
      "Epoch 1/10  Iteration 148/1780 Training loss: 3.1948 16.0500 sec/batch\n",
      "Epoch 1/10  Iteration 149/1780 Training loss: 3.1928 17.1638 sec/batch\n",
      "Epoch 1/10  Iteration 150/1780 Training loss: 3.1908 15.3830 sec/batch\n",
      "Epoch 1/10  Iteration 151/1780 Training loss: 3.1889 16.3876 sec/batch\n",
      "Epoch 1/10  Iteration 152/1780 Training loss: 3.1871 15.6832 sec/batch\n",
      "Epoch 1/10  Iteration 153/1780 Training loss: 3.1850 16.1573 sec/batch\n",
      "Epoch 1/10  Iteration 154/1780 Training loss: 3.1831 16.4438 sec/batch\n",
      "Epoch 1/10  Iteration 155/1780 Training loss: 3.1810 17.2456 sec/batch\n",
      "Epoch 1/10  Iteration 156/1780 Training loss: 3.1789 18.2348 sec/batch\n",
      "Epoch 1/10  Iteration 157/1780 Training loss: 3.1766 16.5922 sec/batch\n",
      "Epoch 1/10  Iteration 158/1780 Training loss: 3.1744 16.5025 sec/batch\n",
      "Epoch 1/10  Iteration 159/1780 Training loss: 3.1721 16.7439 sec/batch\n",
      "Epoch 1/10  Iteration 160/1780 Training loss: 3.1700 15.8496 sec/batch\n",
      "Epoch 1/10  Iteration 161/1780 Training loss: 3.1677 16.3567 sec/batch\n",
      "Epoch 1/10  Iteration 162/1780 Training loss: 3.1653 16.3684 sec/batch\n",
      "Epoch 1/10  Iteration 163/1780 Training loss: 3.1628 17.6444 sec/batch\n",
      "Epoch 1/10  Iteration 164/1780 Training loss: 3.1604 15.8806 sec/batch\n",
      "Epoch 1/10  Iteration 165/1780 Training loss: 3.1580 15.4422 sec/batch\n",
      "Epoch 1/10  Iteration 166/1780 Training loss: 3.1556 15.8910 sec/batch\n",
      "Epoch 1/10  Iteration 167/1780 Training loss: 3.1532 16.5693 sec/batch\n",
      "Epoch 1/10  Iteration 168/1780 Training loss: 3.1507 16.5602 sec/batch\n",
      "Epoch 1/10  Iteration 169/1780 Training loss: 3.1482 16.3721 sec/batch\n",
      "Epoch 1/10  Iteration 170/1780 Training loss: 3.1455 16.4859 sec/batch\n",
      "Epoch 1/10  Iteration 171/1780 Training loss: 3.1430 17.4673 sec/batch\n",
      "Epoch 1/10  Iteration 172/1780 Training loss: 3.1406 16.7851 sec/batch\n",
      "Epoch 1/10  Iteration 173/1780 Training loss: 3.1383 16.6318 sec/batch\n",
      "Epoch 1/10  Iteration 174/1780 Training loss: 3.1360 16.7518 sec/batch\n",
      "Epoch 1/10  Iteration 175/1780 Training loss: 3.1334 16.7011 sec/batch\n",
      "Epoch 1/10  Iteration 176/1780 Training loss: 3.1308 16.7527 sec/batch\n",
      "Epoch 1/10  Iteration 177/1780 Training loss: 3.1282 17.8590 sec/batch\n",
      "Epoch 1/10  Iteration 178/1780 Training loss: 3.1255 16.0838 sec/batch\n",
      "Epoch 2/10  Iteration 179/1780 Training loss: 2.7063 16.4863 sec/batch\n",
      "Epoch 2/10  Iteration 180/1780 Training loss: 2.6598 16.2049 sec/batch\n",
      "Epoch 2/10  Iteration 181/1780 Training loss: 2.6481 17.1478 sec/batch\n",
      "Epoch 2/10  Iteration 182/1780 Training loss: 2.6397 16.1879 sec/batch\n",
      "Epoch 2/10  Iteration 183/1780 Training loss: 2.6345 15.9465 sec/batch\n",
      "Epoch 2/10  Iteration 184/1780 Training loss: 2.6275 15.7921 sec/batch\n",
      "Epoch 2/10  Iteration 185/1780 Training loss: 2.6245 18.3554 sec/batch\n",
      "Epoch 2/10  Iteration 186/1780 Training loss: 2.6209 16.5210 sec/batch\n",
      "Epoch 2/10  Iteration 187/1780 Training loss: 2.6174 16.5704 sec/batch\n",
      "Epoch 2/10  Iteration 188/1780 Training loss: 2.6125 15.7797 sec/batch\n",
      "Epoch 2/10  Iteration 189/1780 Training loss: 2.6075 16.9965 sec/batch\n",
      "Epoch 2/10  Iteration 190/1780 Training loss: 2.6043 16.9910 sec/batch\n",
      "Epoch 2/10  Iteration 191/1780 Training loss: 2.6006 15.7720 sec/batch\n",
      "Epoch 2/10  Iteration 192/1780 Training loss: 2.6004 17.5903 sec/batch\n",
      "Epoch 2/10  Iteration 193/1780 Training loss: 2.5967 16.0922 sec/batch\n",
      "Epoch 2/10  Iteration 194/1780 Training loss: 2.5937 17.5657 sec/batch\n",
      "Epoch 2/10  Iteration 195/1780 Training loss: 2.5908 16.0012 sec/batch\n",
      "Epoch 2/10  Iteration 196/1780 Training loss: 2.5899 17.6354 sec/batch\n",
      "Epoch 2/10  Iteration 197/1780 Training loss: 2.5869 16.9629 sec/batch\n",
      "Epoch 2/10  Iteration 198/1780 Training loss: 2.5830 17.4435 sec/batch\n",
      "Epoch 2/10  Iteration 199/1780 Training loss: 2.5797 16.5899 sec/batch\n",
      "Epoch 2/10  Iteration 200/1780 Training loss: 2.5779 16.5420 sec/batch\n",
      "Validation loss: 2.43025 Saving checkpoint!\n",
      "Epoch 2/10  Iteration 201/1780 Training loss: 2.5758 20.7506 sec/batch\n",
      "Epoch 2/10  Iteration 202/1780 Training loss: 2.5725 16.8527 sec/batch\n",
      "Epoch 2/10  Iteration 203/1780 Training loss: 2.5693 21.9442 sec/batch\n",
      "Epoch 2/10  Iteration 204/1780 Training loss: 2.5667 18.0095 sec/batch\n",
      "Epoch 2/10  Iteration 205/1780 Training loss: 2.5640 16.7424 sec/batch\n",
      "Epoch 2/10  Iteration 206/1780 Training loss: 2.5612 14.1765 sec/batch\n",
      "Epoch 2/10  Iteration 207/1780 Training loss: 2.5590 15.5506 sec/batch\n",
      "Epoch 2/10  Iteration 208/1780 Training loss: 2.5567 17.5351 sec/batch\n",
      "Epoch 2/10  Iteration 209/1780 Training loss: 2.5549 19.4075 sec/batch\n",
      "Epoch 2/10  Iteration 210/1780 Training loss: 2.5521 16.1992 sec/batch\n",
      "Epoch 2/10  Iteration 211/1780 Training loss: 2.5493 17.5383 sec/batch\n",
      "Epoch 2/10  Iteration 212/1780 Training loss: 2.5472 16.6519 sec/batch\n",
      "Epoch 2/10  Iteration 213/1780 Training loss: 2.5446 18.6175 sec/batch\n",
      "Epoch 2/10  Iteration 214/1780 Training loss: 2.5428 16.2835 sec/batch\n",
      "Epoch 2/10  Iteration 215/1780 Training loss: 2.5403 16.6907 sec/batch\n",
      "Epoch 2/10  Iteration 216/1780 Training loss: 2.5374 16.3727 sec/batch\n",
      "Epoch 2/10  Iteration 217/1780 Training loss: 2.5348 16.1202 sec/batch\n",
      "Epoch 2/10  Iteration 218/1780 Training loss: 2.5322 17.0664 sec/batch\n",
      "Epoch 2/10  Iteration 219/1780 Training loss: 2.5298 15.9216 sec/batch\n",
      "Epoch 2/10  Iteration 220/1780 Training loss: 2.5273 16.3666 sec/batch\n",
      "Epoch 2/10  Iteration 221/1780 Training loss: 2.5246 15.4590 sec/batch\n",
      "Epoch 2/10  Iteration 222/1780 Training loss: 2.5220 16.4352 sec/batch\n",
      "Epoch 2/10  Iteration 223/1780 Training loss: 2.5195 16.0690 sec/batch\n",
      "Epoch 2/10  Iteration 224/1780 Training loss: 2.5166 15.8321 sec/batch\n",
      "Epoch 2/10  Iteration 225/1780 Training loss: 2.5149 15.8166 sec/batch\n",
      "Epoch 2/10  Iteration 226/1780 Training loss: 2.5127 16.1654 sec/batch\n",
      "Epoch 2/10  Iteration 227/1780 Training loss: 2.5108 15.7645 sec/batch\n",
      "Epoch 2/10  Iteration 228/1780 Training loss: 2.5091 16.0279 sec/batch\n",
      "Epoch 2/10  Iteration 229/1780 Training loss: 2.5067 15.3727 sec/batch\n",
      "Epoch 2/10  Iteration 230/1780 Training loss: 2.5050 16.3202 sec/batch\n",
      "Epoch 2/10  Iteration 231/1780 Training loss: 2.5030 15.9633 sec/batch\n",
      "Epoch 2/10  Iteration 232/1780 Training loss: 2.5010 15.6320 sec/batch\n",
      "Epoch 2/10  Iteration 233/1780 Training loss: 2.4988 19.9294 sec/batch\n",
      "Epoch 2/10  Iteration 234/1780 Training loss: 2.4972 17.5924 sec/batch\n",
      "Epoch 2/10  Iteration 235/1780 Training loss: 2.4953 17.6436 sec/batch\n",
      "Epoch 2/10  Iteration 236/1780 Training loss: 2.4933 15.8713 sec/batch\n",
      "Epoch 2/10  Iteration 237/1780 Training loss: 2.4912 16.3558 sec/batch\n",
      "Epoch 2/10  Iteration 238/1780 Training loss: 2.4896 17.1457 sec/batch\n",
      "Epoch 2/10  Iteration 239/1780 Training loss: 2.4876 16.8192 sec/batch\n",
      "Epoch 2/10  Iteration 240/1780 Training loss: 2.4861 15.6096 sec/batch\n",
      "Epoch 2/10  Iteration 241/1780 Training loss: 2.4847 16.6330 sec/batch\n",
      "Epoch 2/10  Iteration 242/1780 Training loss: 2.4830 16.6429 sec/batch\n",
      "Epoch 2/10  Iteration 243/1780 Training loss: 2.4810 17.5801 sec/batch\n",
      "Epoch 2/10  Iteration 244/1780 Training loss: 2.4797 16.8225 sec/batch\n",
      "Epoch 2/10  Iteration 245/1780 Training loss: 2.4780 16.8570 sec/batch\n",
      "Epoch 2/10  Iteration 246/1780 Training loss: 2.4760 16.1997 sec/batch\n",
      "Epoch 2/10  Iteration 247/1780 Training loss: 2.4741 15.9175 sec/batch\n",
      "Epoch 2/10  Iteration 248/1780 Training loss: 2.4724 16.5020 sec/batch\n",
      "Epoch 2/10  Iteration 249/1780 Training loss: 2.4710 16.8560 sec/batch\n",
      "Epoch 2/10  Iteration 250/1780 Training loss: 2.4695 16.1395 sec/batch\n",
      "Epoch 2/10  Iteration 251/1780 Training loss: 2.4680 15.5308 sec/batch\n",
      "Epoch 2/10  Iteration 252/1780 Training loss: 2.4662 16.4548 sec/batch\n",
      "Epoch 2/10  Iteration 253/1780 Training loss: 2.4644 16.3002 sec/batch\n",
      "Epoch 2/10  Iteration 254/1780 Training loss: 2.4632 15.8637 sec/batch\n",
      "Epoch 2/10  Iteration 255/1780 Training loss: 2.4616 15.4897 sec/batch\n",
      "Epoch 2/10  Iteration 256/1780 Training loss: 2.4601 16.5974 sec/batch\n",
      "Epoch 2/10  Iteration 257/1780 Training loss: 2.4583 16.4280 sec/batch\n",
      "Epoch 2/10  Iteration 258/1780 Training loss: 2.4568 15.7208 sec/batch\n",
      "Epoch 2/10  Iteration 259/1780 Training loss: 2.4550 15.8954 sec/batch\n",
      "Epoch 2/10  Iteration 260/1780 Training loss: 2.4536 16.5063 sec/batch\n",
      "Epoch 2/10  Iteration 261/1780 Training loss: 2.4519 15.8956 sec/batch\n",
      "Epoch 2/10  Iteration 262/1780 Training loss: 2.4503 15.6665 sec/batch\n",
      "Epoch 2/10  Iteration 263/1780 Training loss: 2.4481 16.2262 sec/batch\n",
      "Epoch 2/10  Iteration 264/1780 Training loss: 2.4465 16.1141 sec/batch\n",
      "Epoch 2/10  Iteration 265/1780 Training loss: 2.4450 16.2063 sec/batch\n",
      "Epoch 2/10  Iteration 266/1780 Training loss: 2.4434 15.4858 sec/batch\n",
      "Epoch 2/10  Iteration 267/1780 Training loss: 2.4417 16.6949 sec/batch\n",
      "Epoch 2/10  Iteration 268/1780 Training loss: 2.4403 17.4130 sec/batch\n",
      "Epoch 2/10  Iteration 269/1780 Training loss: 2.4386 16.1466 sec/batch\n",
      "Epoch 2/10  Iteration 270/1780 Training loss: 2.4373 16.1681 sec/batch\n",
      "Epoch 2/10  Iteration 271/1780 Training loss: 2.4355 17.0661 sec/batch\n",
      "Epoch 2/10  Iteration 272/1780 Training loss: 2.4339 18.1291 sec/batch\n",
      "Epoch 2/10  Iteration 273/1780 Training loss: 2.4322 16.9661 sec/batch\n",
      "Epoch 2/10  Iteration 274/1780 Training loss: 2.4305 16.2747 sec/batch\n",
      "Epoch 2/10  Iteration 275/1780 Training loss: 2.4291 16.6620 sec/batch\n",
      "Epoch 2/10  Iteration 276/1780 Training loss: 2.4276 16.4390 sec/batch\n",
      "Epoch 2/10  Iteration 277/1780 Training loss: 2.4260 16.6324 sec/batch\n",
      "Epoch 2/10  Iteration 278/1780 Training loss: 2.4243 16.2441 sec/batch\n",
      "Epoch 2/10  Iteration 279/1780 Training loss: 2.4231 16.2439 sec/batch\n",
      "Epoch 2/10  Iteration 280/1780 Training loss: 2.4217 15.9510 sec/batch\n",
      "Epoch 2/10  Iteration 281/1780 Training loss: 2.4200 16.1492 sec/batch\n",
      "Epoch 2/10  Iteration 282/1780 Training loss: 2.4184 16.9025 sec/batch\n",
      "Epoch 2/10  Iteration 283/1780 Training loss: 2.4168 15.9378 sec/batch\n",
      "Epoch 2/10  Iteration 284/1780 Training loss: 2.4155 15.9726 sec/batch\n",
      "Epoch 2/10  Iteration 285/1780 Training loss: 2.4140 15.2874 sec/batch\n",
      "Epoch 2/10  Iteration 286/1780 Training loss: 2.4128 17.1385 sec/batch\n",
      "Epoch 2/10  Iteration 287/1780 Training loss: 2.4116 16.4236 sec/batch\n",
      "Epoch 2/10  Iteration 288/1780 Training loss: 2.4100 16.3365 sec/batch\n",
      "Epoch 2/10  Iteration 289/1780 Training loss: 2.4087 15.8220 sec/batch\n",
      "Epoch 2/10  Iteration 290/1780 Training loss: 2.4075 16.1542 sec/batch\n",
      "Epoch 2/10  Iteration 291/1780 Training loss: 2.4060 16.8746 sec/batch\n",
      "Epoch 2/10  Iteration 292/1780 Training loss: 2.4046 15.5709 sec/batch\n",
      "Epoch 2/10  Iteration 293/1780 Training loss: 2.4030 17.0042 sec/batch\n",
      "Epoch 2/10  Iteration 294/1780 Training loss: 2.4014 15.7284 sec/batch\n",
      "Epoch 2/10  Iteration 295/1780 Training loss: 2.4000 16.5529 sec/batch\n",
      "Epoch 2/10  Iteration 296/1780 Training loss: 2.3986 16.0225 sec/batch\n",
      "Epoch 2/10  Iteration 297/1780 Training loss: 2.3974 16.2876 sec/batch\n",
      "Epoch 2/10  Iteration 298/1780 Training loss: 2.3962 15.7271 sec/batch\n",
      "Epoch 2/10  Iteration 299/1780 Training loss: 2.3951 15.6920 sec/batch\n",
      "Epoch 2/10  Iteration 300/1780 Training loss: 2.3936 16.2489 sec/batch\n",
      "Validation loss: 2.14553 Saving checkpoint!\n",
      "Epoch 2/10  Iteration 301/1780 Training loss: 2.3924 17.1392 sec/batch\n",
      "Epoch 2/10  Iteration 302/1780 Training loss: 2.3912 16.5555 sec/batch\n",
      "Epoch 2/10  Iteration 303/1780 Training loss: 2.3900 17.7782 sec/batch\n",
      "Epoch 2/10  Iteration 304/1780 Training loss: 2.3885 17.1725 sec/batch\n",
      "Epoch 2/10  Iteration 305/1780 Training loss: 2.3874 19.5003 sec/batch\n",
      "Epoch 2/10  Iteration 306/1780 Training loss: 2.3862 19.5931 sec/batch\n",
      "Epoch 2/10  Iteration 307/1780 Training loss: 2.3850 17.1516 sec/batch\n",
      "Epoch 2/10  Iteration 308/1780 Training loss: 2.3838 18.4590 sec/batch\n",
      "Epoch 2/10  Iteration 309/1780 Training loss: 2.3824 17.7305 sec/batch\n",
      "Epoch 2/10  Iteration 310/1780 Training loss: 2.3810 15.6125 sec/batch\n",
      "Epoch 2/10  Iteration 311/1780 Training loss: 2.3798 13.7825 sec/batch\n",
      "Epoch 2/10  Iteration 312/1780 Training loss: 2.3786 19.7574 sec/batch\n",
      "Epoch 2/10  Iteration 313/1780 Training loss: 2.3773 24.0648 sec/batch\n",
      "Epoch 2/10  Iteration 314/1780 Training loss: 2.3762 26.8248 sec/batch\n",
      "Epoch 2/10  Iteration 315/1780 Training loss: 2.3750 17.6037 sec/batch\n",
      "Epoch 2/10  Iteration 316/1780 Training loss: 2.3738 16.4988 sec/batch\n",
      "Epoch 2/10  Iteration 317/1780 Training loss: 2.3728 16.4258 sec/batch\n",
      "Epoch 2/10  Iteration 318/1780 Training loss: 2.3716 17.8223 sec/batch\n",
      "Epoch 2/10  Iteration 319/1780 Training loss: 2.3705 15.1291 sec/batch\n",
      "Epoch 2/10  Iteration 320/1780 Training loss: 2.3693 17.8144 sec/batch\n",
      "Epoch 2/10  Iteration 321/1780 Training loss: 2.3680 17.8787 sec/batch\n",
      "Epoch 2/10  Iteration 322/1780 Training loss: 2.3669 13.6695 sec/batch\n",
      "Epoch 2/10  Iteration 323/1780 Training loss: 2.3656 18.5302 sec/batch\n",
      "Epoch 2/10  Iteration 324/1780 Training loss: 2.3646 15.4417 sec/batch\n",
      "Epoch 2/10  Iteration 325/1780 Training loss: 2.3635 16.8143 sec/batch\n",
      "Epoch 2/10  Iteration 326/1780 Training loss: 2.3625 17.1114 sec/batch\n",
      "Epoch 2/10  Iteration 327/1780 Training loss: 2.3613"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "save_every_n = 100\n",
    "train_x, train_y, val_x, val_y = split_data(chars, batch_size, num_steps)\n",
    "\n",
    "model = build_rnn(len(vocab), \n",
    "                  batch_size=batch_size,\n",
    "                  num_steps=num_steps,\n",
    "                  learning_rate=learning_rate,\n",
    "                  lstm_size=lstm_size,\n",
    "                  num_layers=num_layers)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    train_writer = tf.summary.FileWriter('./logs/2/train', sess.graph)\n",
    "    test_writer = tf.summary.FileWriter('./logs/2/test')\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/anna20.ckpt')\n",
    "    \n",
    "    n_batches = int(train_x.shape[1]/num_steps)\n",
    "    iterations = n_batches * epochs\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for b, (x, y) in enumerate(get_batch([train_x, train_y], num_steps), 1):\n",
    "            iteration = e*n_batches + b\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: 0.5,\n",
    "                    model.initial_state: new_state}\n",
    "            summary, batch_loss, new_state, _ = sess.run([model.merged, model.cost, \n",
    "                                                          model.final_state, model.optimizer], \n",
    "                                                          feed_dict=feed)\n",
    "            loss += batch_loss\n",
    "            end = time.time()\n",
    "            print('Epoch {}/{} '.format(e+1, epochs),\n",
    "                  'Iteration {}/{}'.format(iteration, iterations),\n",
    "                  'Training loss: {:.4f}'.format(loss/b),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "            \n",
    "            train_writer.add_summary(summary, iteration)\n",
    "        \n",
    "            if (iteration%save_every_n == 0) or (iteration == iterations):\n",
    "                # Check performance, notice dropout has been set to 1\n",
    "                val_loss = []\n",
    "                new_state = sess.run(model.initial_state)\n",
    "                for x, y in get_batch([val_x, val_y], num_steps):\n",
    "                    feed = {model.inputs: x,\n",
    "                            model.targets: y,\n",
    "                            model.keep_prob: 1.,\n",
    "                            model.initial_state: new_state}\n",
    "                    summary, batch_loss, new_state = sess.run([model.merged, model.cost, \n",
    "                                                               model.final_state], feed_dict=feed)\n",
    "                    val_loss.append(batch_loss)\n",
    "                    \n",
    "                test_writer.add_summary(summary, iteration)\n",
    "\n",
    "                print('Validation loss:', np.mean(val_loss),\n",
    "                      'Saving checkpoint!')\n",
    "                #saver.save(sess, \"checkpoints/anna/i{}_l{}_{:.3f}.ckpt\".format(iteration, lstm_size, np.mean(val_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints/anna')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "Now that the network is trained, we'll can use it to generate new text. The idea is that we pass in a character, then the network will predict the next character. We can use the new one, to predict the next one. And we keep doing this to generate all new text. I also included some functionality to prime the network with some text by passing in a string and building up a state from that.\n",
    "\n",
    "The network gives us predictions for each character. To reduce noise and make things a little less random, I'm going to only choose a new character from the top N most likely characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    prime = \"Far\"\n",
    "    samples = [c for c in prime]\n",
    "    model = build_rnn(vocab_size, lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "checkpoint = \"checkpoints/anna/i3560_l512_1.122.ckpt\"\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "checkpoint = \"checkpoints/anna/i200_l512_2.432.ckpt\"\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "checkpoint = \"checkpoints/anna/i600_l512_1.750.ckpt\"\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "checkpoint = \"checkpoints/anna/i1000_l512_1.484.ckpt\"\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
